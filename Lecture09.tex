
%\documentclass[mathserif,aspectratio=1610]{beamer}
\documentclass[mathserif,handout,aspectratio=1610]{beamer}
%\usepackage{beamerthemeshadow}

\input{$HOME/tex/inputs/defsSkript}%$
\input{$HOME/tex/inputs/styleBeamerVkoekMa}%$
%\input{styleBeamerVkoekMa}%$

\usepackage{graphicx}

%\newcommand{\pathDiscrChoice}{$HOME/vorlesungen/Verkehrsoekonometrie_Ma/discrChoice_cc_Levmar} %$
\newcommand{\pathDiscrChoice}{figsDiscr} 

%##############################################################

\begin{document}

\section{Chapter 9: Inferential Statistics I: Maximum-Likelihood Estimation}

%###################################################
\frame{  %title layout
%###################################################
\makePale{1.00}{0.50}{0.70}{1.40}{1.40}

\placebox{0.50}{0.45}{
  \figSimple{1.40\textwidth}{figsDiscr/RC_skript_4alt_r_eng_corrMatrix.png}}
%makePale{opacity}{centerXrel}{centerYrel}{wrel}\{hrel} 
\makePale{0.80}{0.50}{0.70}{1.40}{1.40}

\placebox{0.50}{0.82}{
\parbox{0.6\textwidth}{
\myheading{9: Inferential Statistics I}\\
\myheading{\hspace*{1.0em} of Discrete-Choice Models:}\\
\myheading{\hspace*{1.0em} Maximum-Likelihood Estimation}
}}

\placebox{0.50}{0.44}{\parbox{0.6\textwidth}{
{\large
\bi
\item 9.1. Maximum-Likelihood Estimation
\item 9.2 Estimation Errors: Variance-Covariance Matrix
\bi
\item 9.2.1 Example 1: SP Survey in the Audience
\item 9.2.2 Example 2: RP Survey in the Audience
\ei
\ei
}
}}

}




\subsection{9.1. Maximum-Likelihood Estimation}

%###################################################
\frame{\frametitle{9.1. Maximum-Likelihood Estimation: 
the likelihood function}
%###################################################
\bi
\item The \bfdef{maximum-likelihood (ML)} estimation is applicable for
general stochastic models where the probabilities depend on a
 parameter vector
$\vecbeta$
\pause \item The goal is to maximize the \bfdef{likelihood function
  $L(\vecbeta)$}, i.e., the probability that the model predicts
  \emph{all} data points $(\vec{y}_n, \vec{x}_n)$, $n=1, ..., N$:
\maindm{
L(\vecbeta)=P\big(\ \hat{\vec{y}}_1(\vecbeta)=\vec{y}_1, \quad ..., \quad
\hat{\vec{y}}_N(\vecbeta)=\vec{y}_N\ \big)
}
where $\hat{\vec{y}}_n=\hat{\vec{y}}(\vec{x}_n)$ 
gives the model estimate for $\vec{x}_n$
\pause \item For continuous endogenous variables, the likelihood function is
  given by the multi-dimensional probability density at the data
  points:
\bdm
L(\vecbeta)=f_{\hat{\vec{y}}_1(\vecbeta), ..., \hat{\vec{y}}_N(\vecbeta)}
(\vec{y}_1, ..., \vec{y}_N)
\edm

\pause \itemAsk \colAsk{Verify that the density formulation is equivalent to
  the probability definition by requiring the model estimations to
  be in small intervals around the data instead of hitting the data exactly.} 
\pause \itemAnswer \colAnswer{The multi-dimensional probability density $f(.)$
  is defined such that 
$\diff{P}=f_{\hat{\vec{y}}_1, ..., \hat{\vec{y}}_N}(\vec{y}) 
\text{d}^N \vec{y}$. Keeping $\text{d}^N \vec{y}$ small and constant,
$\diff{P}$ and thus $P$ is maximized if and only if $f(.)$ is maximized.}
\ei
}

%###################################################
\frame{\frametitle{Maximum-likelihood estimation}
%###################################################
\bi
\item The ML method maximizes the likelihood function:
\maindm{
\hatvecbeta=\text{arg} \max_{\vecbeta} L(\vecbeta)
}

\pause \item Equivalently, and often better, one maximizes the
  \bfdef{log-likelihood}:
\maindm{
\hatvecbeta=\text{arg} \max_{\vecbeta} \tilL(\vecbeta), \quad
\tilL(\vecbeta)=\ln L(\vecbeta)}
\vspace{2em}

\pause \itemAsk \colAsk{Why it does not matter whether to maximize the
  likelihood or the log-likelihood?}
\pause \itemAnswer \colAnswer{Since, as a probability or probability
  density, $L>0$ and the log function is defined and strictly monotonously
  increasing in this range. Since (i) in this case
\bdm
x>y \Leftrightarrow f(x) > f(y)
\edm
(ii) the maximum function is based on this inequality
  relation, the \emph{argument} of the maximum remains unchanged.}
\ei
}



%###################################################
\frame{\frametitle{Application 1: Regression models}
%###################################################

Besides OLS, the ML can also be used to estimate regression
models. Does it give the same result, at least if the statistical Gau\3-Markow
  conditions are  satisfied? 
\bdma
\visible<2->{L(\vecbeta) &\stackrel{\text{$\epsilon_n$ independent}}{=}& 
 \prod_{n=1}^N f_n(y_n)}
\visible<3->{ \stackrel{\epsilon_n\sim i.d. N(0,\sigma^2)}{=} 
 \prod_{n=1}^N\frac{1}{\sqrt{2\pi\sigma^2}}
\exp\left[-\frac{(y_n-\vec{\beta}\vec{x}_n)^2}{2\sigma^2}\right],\\}
\visible<4->{\tilL(\vecbeta) &=& \sum_{n=1}^N \ln f_n(y_n)
 = \sum_{n=1}^N\left\{
 -\frac{1}{2}(\ln 2\pi+\ln \sigma^2)
 -\left[\frac{(y_n-\vec{\beta}\vec{x}_n)^2}{2\sigma^2}\right]\right\}
 \nonumber\\}
 \visible<5->{&=& -\frac{N}{2}(\ln 2\pi+\ln \sigma^2)
 -\frac{1}{2\sigma^2}(\vec{y}-\m{X}\vec{\beta})\tr(\vec{y}-\m{X}\vec{\beta})}
\edma
\visible<6->{Except for the irrelevant additive and multiplicative
  constants, this is the SSE function  of the 
OLS method and therefore leads to the same estimator!}

\vspace{1em}

\bi
\visible<7->{\itemAsk \colAsk{Why it is possible 
to express $L(\vecbeta)$ as a product?}}
\visible<8->{\itemAnswer \colAnswer{Since the random terms $\epsilon_n
    \sim i.i.d 
  N(0,\sigma^2)$, particularly, they are \emph{independent} from each
  other}}
\ei
}

%###################################################
\frame{\frametitle{Application 2: Discrete-choice models}
%###################################################
\bi
\item Probability to predict the chosen alternative $i_n$ for a
  \emph{single} decision $n$:
\bdma
P\left(\hatvec{Y}_{n}=\vec{y}_n\right)
&=& P\left(\hat{Y}_{n1}=y_{n1}, \ ..., \ \hat{Y}_{nI}=y_{nI}\right)\\
\visible<2->{&=& \prod_{i=1}^I
\left[P_{ni}(\vecbeta)\right]^{y_{ni}}=P_{ni_n}(\vecbeta)
\edma
(this relies on the exclusivity/completeness
of ${\cal A}_n$ and of independent RUs)
% but the product expression is
%more suitable for the further development)
}
\\[1em]
\visible<3->{\pause \item Probability to predict \emph{all} the
  decisions correctly assuming
  independent  decisions:
\bdma
\visible<3->{L(\vecbeta) &=&
P \left(\vec{Y}\!_{1}(\vecbeta)=\vec{y}_1,
...,\vec{Y}\!_{N}(\vecbeta)=\vec{y}_N \right)\\}
\visible<4->{ &=&  \prod_{n=1}^N \prod_{i=1}^I
\left[P_{ni}(\vecbeta)\right]^{y_{ni}}}
\edma
}
\ei
\vspace{-2em}

\visible<5->{ML estimation:
\maindm{\hatvecbeta=\text{arg} \max_{\vec{\beta}}\tilL(\vecbeta), \quad
 \tilL(\vecbeta)
=\sum\limits_{n=1}^N  
\sum\limits_{i=1}^I y_{ni} \ln P_{ni}(\vec{\beta})
=\sum\limits_{n=1}^N \ln P_{ni_n}(\vec{\beta})
}}

}

%###################################################
\frame{\frametitle{Question}
%###################################################
\bi
\itemAsk \parbox[t]{0.65\textwidth}{\vspace{-0.9em}

\colAsk { Show that, in deriving the main ML result
$\tilL=\sum_n\sum_iy_{ni}\ln P_{ni}$, the random utilities need not to
be uncorrelated between alternatives, only between choices}}
\vspace{2em}

\pause \itemAnswer \parbox[t]{0.65\textwidth}{\vspace{-0.9em}

\colAnswer{Because of the exclusivity/completeness
  requirement for the alternatives, exactly one alternative can be
  chosen per decision so it is enough to maximize the corresponding
  probability (which, of course, depends on possible correlations)}}

\ei
}

%###################################################
\frame{\frametitle{Estimating models with only ACs}
%###################################################

If there are no exogenous variables, we are left with just the ACs
reflecting that people prefer certain alternatives over others for
unknown reasons:
\bdm
V_{ni}=\sum_{m=1}^{I-1}\beta_m\delta_{mi} \quad \text{or} \quad
V_{ni}=\beta_i \ \text{if} \ i\neq I, \ V_{nI}=0
\edm
This \bfdef{AC-only model} 
will be the ``reference case'' when estimating the
model quality, e.g., by the \bfdef{likelihood-ratio index}.

{\small
\bi
\pause \itemAsk \colAsk{ Show that the estimated models gives probabilities
  $P_{ni}=P_i$ that are equal to the observed choice fractions $N_i/N$.
  (\emph{Hint:} Lagrange multiplicators to satisfy $\sum_iP_i=1$)}

\pause \itemAnswer \colAnswer{we have 
$\tilL(\vec{P})=\sum_n \ln P_{i_n}=\sum_iN_i\ln P_i$; \ maximize under
  the constraint $\sum_iP_i=1$:
\bdm
\abl{}{P_i}\left(\tilL(\vec{P})-\lambda(\sum_iP_i-1)\right)\stackrel{!}{=}0 
\ \Rightarrow \ \frac{N_i}{P_i}=\lambda \Rightarrow \ P_i \propto N_i 
\edm
}

\pause \itemAsk \colAsk{ Based on this result $P_i=N_i/N$, give the
  parameters for the 
 AC-only MNL and for the binary i.i.d. Probit model}
\pause \colAnswer{Logit: $P_i/P_I=N_i/N_I=\exp(\beta_i)$ (notice that
  $I$ is the reference w/o AC)}
\ei
}

}



%###################################################
\frame{\frametitle{Exercise: simple binomial model with an AC and
    travel time}
%###################################################

\placebox{0.50}{0.84}{$V_{ni}=\beta_1\delta_{i1}+\beta_2T_{ni}$}


\visible<2->{
\placebox{0.25}{0.25}
 {\figSimple{0.51\textwidth}{figsDiscr/kalLogitBinom_beta1beta2.png}}

\placebox{0.75}{0.25}
 {\figSimple{0.51\textwidth}{figsDiscr/kalProbitBinom_beta1beta2.png}}
}


\visible<1->{
\placebox{0.50}{0.65}{\parbox{\textwidth}{
{\footnotesize
\begin{center}
\begin{tabular}{|c||c|c|c|c|} \hline
Choice set
 & $T\sub{ped}=T_1$ [min]
 & $T\sub{bike}=T_2$ [min]
 & \# chosen 1
 & \# chosen 2 \\ \hline
1 & 15 & 30 & 3 & 2 \\
2 & 10 & 15 & 2 & 3 \\
3 & 20 & 20 & 1 & 4 \\
4 & 30 & 25 & 1 & 4 \\
5 & 30 & 20 & 0 & 5 \\
6 & 60 & 30 & 0 & 5 \\ \hline
\end{tabular}
\end{center}
}}}}

}

%###################################################
\frame{\frametitle{I: Graphical solution}
%###################################################

\visible<1>{\placebox{0.26}{0.54}
 {\figSimple{0.54\textwidth}{figsDiscr/kalLogitBinom_beta1beta2.png}}}

\visible<2->{\placebox{0.26}{0.54}
 {\figSimple{0.54\textwidth}{figsDiscr/kalLogitBinom_beta1beta2_graph.png}}}

\visible<3>{\placebox{0.74}{0.54}
 {\figSimple{0.54\textwidth}{figsDiscr/kalProbitBinom_beta1beta2.png}}}

\visible<4->{\placebox{0.74}{0.54}
 {\figSimple{0.54\textwidth}{figsDiscr/kalProbitBinom_beta1beta2_graph.png}}}

\visible<1->{\placebox{0.50}{0.84}{$V_{ni}=\beta_1\delta_{i1}+\beta_2T_{ni}$}}

\visible<2->{\placebox{0.30}{0.14}{\parbox{0.4\textwidth}{
\textbf{Logit}:\\[1ex]
$\tilL=-12$\\
$\hatbeta_1=-1.3, \ \hatbeta_2=-0.14,$\\
$\text{AC in minutes: } -\, \frac{\hatbeta_1}{\hatbeta_2}=\unit[-9]{min}$
}}}

\visible<4->{\placebox{0.80}{0.14}{\parbox{0.4\textwidth}{
\textbf{Probit}:\\[1ex]
$\tilL=-12$\\
$\hatbeta_1=-1.1, \ \hatbeta_2=-0.12,$\\
$\text{AC in minutes: } -\, \frac{\hatbeta_1}{\hatbeta_2}=\unit[-9]{min}$
}}}

}



%###################################################
\frame{\frametitle{II. Numerical solution}
%###################################################

\bi
\item
Generally, we have a nonlinear optimization problem. 
\item For
parameter-linear utilities, we know for the MNL that a maximum exists
and is unique.
\pause \item Standard methods of nonlinear optimization are possible:
\bi
\item \bfdef{Newton's and quasi-Newton method}: Fast but may be unstable
\item \bfdef{Gradient/steepest descent methods}: slow but reliable
\item \bfdef{Broyden-Fletcher-Goldfarb-Shanno (BFGS)} or 
\bfdef{Levenberg-Marquardt algorithm} combining gradient and Newton
methods. Such methods are used in many software packages
\item \bfdef{genetic algorithms} if the \emph{objective function
  landscape} is complicated (nonlinear utilities).
\ei
\ei
}

%###################################################
\frame{\frametitle{Special case: estimating the MNL}
%###################################################

The special structure of the MNL with parameter-linear utilities,
$V_{ni}=\sum_m \beta_mX_{mni}$ allows for an intuitive formulation of
the estimation problem: 
\maintextbox{0.8\textwidth}{The observed and modeled 
 \bfdef{property sums} sums of the
  factors $X$ for a given parameter $m$ should be the same
\bdma
X_m\sup{MNL} &=& X_m\sup{data}, \\
\sum_{n, i} x_{nmi}\, P_{ni}(\hatvecbeta) &=&
\sum_{n, i} x_{nmi}\, y_{ni} = \sum_n x_{nmi_n}
\edma
\vspace{-1em}
}

}


%###################################################
\frame{\frametitle{Example: four factors, two alternatives}
%###################################################
MNL model,
$V_{ni}=\beta_1T_{ni}+\beta_2C_{ni}+\beta_3 g_i\delta_{i1}+\beta_4\delta_{i1}$,
$g_{\malepng}=0$, $g_{\femalepng}=1$:
\vspace{1em}
{\small


\bi
\item $X_1=T$: Total travel time for the chosen alternatives:
\bdm
T\sup{MNL} = \sum_{n,i} P_{ni}(\vecbeta)T_{ni}, \quad
T\sup{data} = \sum_{n,i} y_{ni} T_{ni}=\sum_nT_{ni_n}
\edm

\pause \item $X_2=C$: Total money spent by the decision makers:
\bdm
C\sup{MNL} = \sum_{n,i} P_{ni}(\vecbeta)C_{ni}, \quad
C\sup{data} = \sum_{n,i} y_{ni} C_{ni}=\sum_nC_{ni_n}
\edm

\pause \item $X_3=N_{1,\femalepng}$: number of woman choosing alternative 1:
\bdm
N_{1,\femalepng}\sup{MNL} = \sum_{n}  P_{n1}(\vecbeta)g_n, \quad
N_{1,\femalepng}\sup{data} = \sum_{n} y_{n1} g_n 
\edm

\pause \item $X_4=N_1$: total number of persons choosing alternative 1:
\bdm
N_1\sup{MNL} = \sum_{n}  P_{n1}(\vecbeta), \quad
N_1\sup{data} = \sum_{n} y_{n1}
\edm

\ei
}


}

\subsection{9.2 Estimation errors}

%###################################################
\frame{\frametitle{9.2 Estimation Errors: Variance-Covariance Matrix}
%###################################################

{\small
Since the log-likelihood is maximized at $\hatvecbeta$, we have
\bdm
\ablpart{\tilL}{\vecbeta}=0 \ \Rightarrow \ 
\tilL(\vec{\beta}) \approx \tilL\sub{max} + \frac{1}{2}
\Delta \vec{\beta}^{\,T} \cdot \m{H} \cdot \Delta \vec{\beta}, \quad
\Delta \vec{\beta}=\vecbeta-\hatvecbeta
\edm
with the (negative definite) 
Hessian $H_{lm}=\left.\ablpartmix{\tilL(\vecbeta)}{\beta_l}{\beta_m}
\right|_{\vecbeta=\hatvecbeta}$

\pause
Compare $L(\vec{\beta})$ near
its maximum with the density $f(\vec{x})$ of the general multivariate normal
distribution with 
variance-covariance matrix $\vec{\Sigma}$:
\bdma
L(\vec{\beta}) &=& L\sub{max}\exp\left( 
\frac{1}{2} \Delta \vec{\beta}^{\,T} \cdot \m{H} \cdot \Delta
\vec{\beta}\right),\\[-1ex]
f(\vec{x}) &=& \left((2\pi)^{M} \text{Det}\vec{\Sigma}\right)^{-1/2} \ 
\exp\left(-\frac{1}{2} \vec{x}\tr\vec{\Sigma}^{-1}\, \vec{x}\right)
\edma
\vspace{-1ex}

\pause Identify $\Delta \vecbeta$ with $\vec{x}$, the sought-after
variance-covariance matrix $\m{V}$ with $\vec{\Sigma}$,
  and assume the 
asymptotic limit (higher than quadratic terms in $\tilL(\hatvecbeta)$
negligible): \ $\Rightarrow$
}

\maindm{\m{V}=\text{Cov}(\hatvecbeta)
=E\left[\left(\vecbeta-\hatvecbeta\right)
\left(\vecbeta-\hatvecbeta\right)\tr\right] 
\approx -\m{H}^{-1}(\hatvecbeta)
}
}

%###################################################
\frame{\frametitle{Fisher's information matrix}
%###################################################
{\small
The variance-covariance matrix is related to 
\bfdef{Fisher's information matrix $\vec{\cal I}$}:
\bdm
\vec{\cal I}=\m{V}^{-1}=-\m{H}, \quad
I_{lm}=-\ablpartmix{\tilL(\hatvecbeta)}{\beta_l}{\beta_m}
\edm
\bi
\item
Roughly speaking, information is missing uncertainty, so the higher
the main components of $\vec{\cal I}$, the lower the main components
of $\m{V}$
\vspace{1ex}
\pause \item
\bfdef{Cram{\'e}r-Rao inequality:} A lower bound for the
variance-covariance matrix is the inverse of Fisher's information
matrix
$\Rightarrow$ The ML estimator is \bfdef{asymptotically efficient}
\vspace{1ex}

\pause \item Comparison with the OLS estimator 
$\m{V}\sub{OLS}=2\sigma^2 \m{H}\sub{SSE}^{-1}$ of regression models:
\bdm
 \vec{\cal I}=-\m{H}=\m{H}\sub{SSE}/(2\sigma^2)=\m{X}\tr\m{X}/\sigma^2
\edm
The negative Hesse matrix of $\tilL(\vecbeta)$ 
 is proportional to the Hesse matrix of the regression SSE $S(\vecbeta)$.
\ei
}
}




\subsection{9.2.1 Example 1: SP Survey in the Audience}
%~/vorlesungen/Verkehrsoekonometrie_Ma/discrChoice_cc_Levmar/SC_WS1819_3alt_globalT_fProb

%###################################################
\frame{\label{sec:SP}\frametitle{9.2.1 Example 1 from past lecture:\\ 
SP Survey in the Audience WS18/19 \red{(red: bad weather, $\bfmath{W=1}$)}}
%###################################################

\vspace{0.5em}
{\small
\hspace{5em}
\begin{tabular}{|r||c|c|c||r|r|r|}
\hline 
\myBox{2.5em}{Choice\\Set} &
  \myBox{3em}{Alt. 1:\\Ped} &
  \myBox{3em}{Alt. 2:\\Bike} &
  \myBox{3em}{Alt. 3:\\ PT/Car} &
  Alt 1 & Alt 2 & Alt 3 \\ \hline\hline
1  & 30\,min & 20\,min & 20\,min+0\euro{} & 1  & 3 & 7 \\[0.1em] \hline
2  & 30\,min & 20\,min & 20\,min+2\euro{} & 2  & 9 & 2\\[0.1em] \hline
3  & 30\,min & 20\,min & 20\,min+1\euro{} & 1  & 5 & 7\\[0.1em] \hline
4  & 30\,min & 20\,min & 30\,min+0\euro{} & 2  & 9 & 3\\[0.1em] \hline
5  & 50\,min & 20\,min & 30\,min+0\euro{} & 0  & 9 & 4\\[0.1em] \hline
6  & 50\,min & 30\,min & 30\,min+0\euro{} & 0  & 3 & 9\\[0.1em] \hline
7  & 50\,min & 40\,min & 30\,min+0\euro{} & 0  & 2 & 10\\[0.1em] \hline
8  & 180\,min & 60\,min & 60\,min+2\euro{} & 0 & 4 & 11\\[0.1em] \hline
9  & 180\,min & 40\,min & 60\,min+2\euro{} & 0 & 9 & 6\\[0.1em] \hline
\red{10}  & \red{180\,min}  & \red{40\,min} & \red{60\,min+2\euro{}}
  & \red{0}  & \red{1}  & \red{14}                   \\[0.1em] \hline
11 & 12\,min & 8\,min & 10\,min+0\euro{} & 3  & 5 & 6\\[0.1em] \hline
12 & 12\,min & 8\,min & 10\,min+1\euro{} & 5  & 7 & 2\\[0.1em] \hline
\end{tabular}
}
}

%###################################################
\frame{\frametitle{Model specification for Model 1 of the past lecture}
%###################################################

\placebox{0.35}{0.43}{
 \figSimple{0.75\textwidth}{\pathDiscrChoice/SC_WS1819_3alt_globalT_fProb.png}
}

\placebox{0.82}{0.77}{\parbox{0.3\textwidth}{
{\small
$
\begin{array}{lll}
 V_i &= & \beta_0\delta_{i1} + \beta_1\delta_{i2}\\
& + & \beta_2 K_i + \beta_{3} T_i
\end{array}
$
}}}


\placebox{0.80}{0.50}{\parbox{0.20\textwidth}{\small\fbox{ 
  $
  \begin{array}{l}
 % \ln L\sub{init}=-176.9,\\
 % \ln L=-141.5, \\
  \beta_0=-0.95 \pm 0.37, \\
  \beta_1=-0.28 \pm 0.24, \\
  \beta_2=+0.17 \pm 0.19, \\
  \beta_3=-0.04 \pm 0.02
  \end{array}
 $
}}}

\placebox{0.82}{0.25}{{\small
   $
   \begin{array}{l}
   \frac{\beta_0}{-\beta_3}=\unit[-22.4]{min},\\[1ex]
   \frac{\beta_1}{-\beta_3}=\unit[-6.6]{min},\\[1ex]
   \frac{60\beta_3}{\beta_2}=\unit[-15]{\text{\euro{}}/h}
   \end{array}
   $
}}

\placebox{0.16}{0.048}{\parbox{0.3\textwidth}{\scriptsize
AIC=275,
BIC=303,\\
$\rho^2=0.200$,
$\tilde{\rho}^2=0.177$
}}

}


%###################################################
\frame{\frametitle{Likelihood and log-likelihood function for varying
    cost ($\beta_2$) and time ($\beta_3$) sensitivities}
%###################################################

\placebox{0.50}{0.76}{\parbox{1.0\textwidth}{\small
  $ 
  V_i= \beta_0\delta_{i1} + \beta_1\delta_{i2} + \beta_2 K
   + \beta_3 T
  $
}}

\placebox{0.25}{0.42}{\parbox{0.55\textwidth}{
  \begin{center}
    \figSimple{0.55\textwidth}{\pathDiscrChoice/SC_WS1819_3alt_globalT_L_beta2_beta3.png}\\
    Likelihood function\\
    $L(\beta_2,\beta_3|\hatbeta_0, \hatbeta_0)$
  \end{center}
}}


\placebox{0.75}{0.42}{\parbox{0.55\textwidth}{
  \begin{center}
    \figSimple{0.55\textwidth}{\pathDiscrChoice/SC_WS1819_3alt_globalT_lnL_beta2_beta3.png}\\
    Log-likelihood function\\
    $\tilde{L}(\beta_2,\beta_3|\hatbeta_0, \hatbeta_1)$
  \end{center}
}}

}
%###################################################
\frame{\frametitle{Log-likelihood function in parameter space}
%###################################################

\placebox{0.50}{0.425}{
 \figSimple{0.88\textwidth}{\pathDiscrChoice/SC_WS1819_3alt_globalT_corrMatrix.png}
}

\placebox{0.65}{0.72}{\small
  $V_i=\beta_0\delta_{i1} + \beta_1\delta_{i2} + \beta_2 K
   + \beta_3 T + \beta_4 W\delta_{i3}
  $
}

}



\subsection{9.2.2 Example 2: RP Survey in the Audience}
% ~/vorlesungen/Verkehrsoekonometrie_Ma/discrChoice_cc_Levmar/RC_skript_4alt_r_eng

%##############################################################
\frame{\frametitle{9.2.2 Example 2: RP Survey in the Audience}
%##############################################################

Distance classes for the trip home to university (cumulated till 2018)
\vspace{0.5em}

Weather: good
\vspace{2em}

{\small
\hspace{3em}
\begin{tabular}{|r|r||c|c|c|c|}
\hline 
\myBox{4em}{Distance} & 
  \myBox{4em}{Class-\\center} &
  \myBox{3em}{Choice\\Alt. 1:\\ped} &
  \myBox{3em}{Choice\\Alt. 2:\\bike} &
  \myBox{3em}{Choice\\Alt. 2:\\PT} &
  \myBox{3em}{Choice\\Alt. 3:\\ car} \\ \hline\hline
\unit[0-1]{km} & \unit[0.5]{km}    &  17 & 16 & 10 & 0\\[0.2em] \hline
\unit[1-2]{km} & \unit[1.5]{km}    &  9  & 23 & 20 & 2\\[0.2em] \hline
\unit[2-5]{km} & \unit[3.5]{km}    &  2  & 27 & 55 & 4\\[0.2em] \hline
\unit[5-10]{km} & \unit[7.5]{km}   &  0  & 7  & 42 & 7\\[0.2em] \hline
\unit[10-20]{km} & \unit[12.5]{km} &  0  & 0  & 18 & 7\\[0.2em] \hline
\end{tabular}}

}



%###################################################
\frame{\frametitle{Revealed Choice: fit quality}
%###################################################

\placebox{0.37}{0.44}{
  \figSimple{0.70\textwidth}{\pathDiscrChoice/RC_skript_4alt_r_eng_fProb.png}
}

\placebox{0.85}{0.72}{\parbox{0.3\textwidth}{\small
$
\begin{array}{l}
V_1=\beta_1+\beta_4 r,\\ 
V_2=\beta_2+\beta_5 r,\\
V_3=\beta_3+\beta_6 r, \\
V_4=0
\end{array}
$
}}

\placebox{0.85}{0.40}{\parbox{0.30\textwidth}{\small
 $
 \begin{array}{l}
 \beta_1= 4.1\pm 0.6, \\
 \beta_2= 3.6\pm 0.5, \\
 \beta_3= 3.0\pm 0.5, \\
 \beta_4=-1.43\pm 0.26, \\
 \beta_5=-0.48\pm 0.08, \\
 \beta_6=-0.14\pm 0.05
 \end{array}
 $
}}

}

%###################################################
\frame{\frametitle{Revealed Choice: Modal split as a function of distance}
%###################################################

\placebox{0.37}{0.44}{
  \figSimple{0.70\textwidth}{\pathDiscrChoice/RC_skript_4alt_r_eng_fProbKumDist.png}
}

\placebox{0.85}{0.72}{\parbox{0.3\textwidth}{\small
$
\begin{array}{l}
V_1=\beta_1+\beta_4 r,\\ 
V_2=\beta_2+\beta_5 r,\\
V_3=\beta_3+\beta_6 r, \\
V_4=0
\end{array}
$
}}

\placebox{0.85}{0.40}{\parbox{0.30\textwidth}{\small
 $
 \begin{array}{l}
 \beta_1= 4.1\pm 0.6, \\
 \beta_2= 3.6\pm 0.5, \\
 \beta_3= 3.0\pm 0.5, \\
 \beta_4=-1.43\pm 0.26, \\
 \beta_5=-0.48\pm 0.08, \\
 \beta_6=-0.14\pm 0.05
 \end{array}
 $
}}


}



%###################################################
\frame{\frametitle{Likelihood and Log-Likelihood as $f(\beta_1,\beta_2)$}

\placebox{0.50}{0.76}{
$V_i=\sum_{m=1}^3\beta_m\delta_{m,i}
+\sum_{m=1}^3\beta_{m+3}\ r\delta_{m,i}$
}

\placebox{0.25}{0.45}{
 \figSimple{0.50\textwidth}{\pathDiscrChoice/RC_skript_4alt_r_eng_L_beta0_beta1.png}
}

\placebox{0.75}{0.45}{
 \figSimple{0.50\textwidth}{\pathDiscrChoice/RC_skript_4alt_r_eng_lnL_beta0_beta1.png}
}

\placebox{0.30}{0.15}{\parbox{0.4\textwidth}{
  Likelihoodfunktion\\
  $L(\beta_1,\beta_2,\hatbeta_3, ...)$
}}

\placebox{0.80}{0.15}{\parbox{0.4\textwidth}{
  Log-Likelihoodfunktion\\
   $\tilL(\beta_1,\beta_2,\hatbeta_3, ...)$
}}

}



%###################################################
\frame{\frametitle{Log-Likelihood: Sections through parameter space}
%###################################################
\figSimple{0.89\textwidth}{\pathDiscrChoice/RC_skript_4alt_r_eng_corrMatrix.png}

{\small
$V_i=\sum_{m=1}^3\beta_m\delta_{m,i}
+\sum_{m=1}^3\beta_{m+3}\ r\delta_{m,i}$
}

}





\end{document}
