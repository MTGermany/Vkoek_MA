%#############################################
% New pdflatex format,
% only png, jpg (or svg?) images but positioning now ok
% template in ~/tex/inputs/template_folien.tex
% [TODO now specialized to Vkoem_Ma; make general]
%#############################################


%\documentclass[mathserif]{beamer}
\documentclass[mathserif,handout]{beamer}
%\documentclass[mathserif,aspectratio=1610]{beamer}
%\documentclass[mathserif,aspectratio=1610,handout]{beamer}
\input{$HOME/tex/inputs/defsSkript}%$
\input{$HOME/tex/inputs/styleBeamerVkoekMa}%$
\usepackage{graphicx}


%##############################################################

\begin{document}

\section{3. Classical Inferential Statistics}

%###################################################
\frame{  %title other layout
%###################################################
\placebox{0.50}{0.50}{
  \figSimple{1.70\textwidth}{figsRegr/regression3dFig.png}}

\makePale{0.85}{0.50}{0.55}{1.20}{1.50}

\placebox{0.50}{0.46}{\parbox{0.6\textwidth}{
\bi
\item[3.1] Expectation and Covariance Matrix of the Ordinary Least
  Squares (OLS) Estimator
\item[3.2] Confidence Intervals
\ei
}}

\placebox{0.50}{0.90}{\myheading{
 Lecture 03: Classical Inferential Statistics I:}}

\placebox{0.50}{0.82}{\myheading{
 Basics and Confidence Intervals}}

}


\subsection{3.1. OLS Expectation and Covariance}

%###################################################
\frame{\frametitle{3.1. Ordinary Least Squares (OLS) Estimator:\\
 Expectation and Covariance}
%###################################################

\bi
\item Only stochasticity: residual errors
  $\veceps$ according to  $\vec{y}=\m{X}\vec{\beta}+\veceps$
\pause \item The OLS estimator is linear in $\vec{y}$:
\bdma
\hatvecbeta 
  &=&
    \left(\m{X}\tr\m{X}\right)^{-1}\m{X}\tr\vec{y} \\ 
 \visible<3->{ &=&
  \left(\m{X}\tr\m{X}\right)^{-1}\m{X}\tr(\m{X}\vec{\beta}+\veceps)\\}
 \visible<4->{ &=&
  \uu{\vec{\beta}+\left(\m{X}\tr\m{X}\right)^{-1} \m{X}\tr\veceps}}
\edma
\ei

\visible<5->{\mysubheading{Expectation value}

\bdm
E(\hatvecbeta)=E(\vecbeta)
 +\left(\m{X}\tr\m{X}\right)^{-1} \m{X}\tr E(\veceps)=\vecbeta
\edm
}


\visible<6->{\maintextbox{0.8\textwidth}{
The OLS estimator of parameter-linear models is \bfdef{unbiased} under
the mild condition $E(\veceps)=\vec{0}$ for all the data points}}

}

 %###################################################
\frame{\frametitle{OLS estimator: variances and covariances}
%###################################################

\bi
\item Gau\3-Markow conditions $\to \epsilon \sim
  \text{i.i.d} N(0,\sigma^2) \to \hatvecbeta $ is normal distributed
\pause \item In this case, the complete error characteristics are
specified by the 
  expectation value and the \bfdef{variance-covariance matrix}
$\m{V}_{\hatvecbeta}$

\ei

\bdma
  \visible<2->{\m{V}_{\hatvecbeta} &\stackrel{\text{def}}{=}&
    E\left( (\hatvecbeta-\vecbeta) (\hatvecbeta-\vecbeta)\tr\right)\\}
  \visible<3->{\remark{insert
      $\hatvecbeta=\vec{\beta}
+\left(\m{X}\tr\m{X}\right)^{-1}\m{X}\tr\veceps$} 
     &=& 
    E\left( (\m{X}\tr\m{X})^{-1}\m{X}\tr\veceps
     \left( (\m{X}\tr\m{X})^{-1}\m{X}\tr\veceps\right)\tr\right) \\}
  \visible<4->{\remark{transpose and inversion rules} &=& 
     E\left( (\m{X}\tr\m{X})^{-1}\m{X}\tr \veceps\veceps\tr
     \m{X}(\m{X}\tr\m{X})^{-1}\right)\\}
  \visible<5->{\remark{$E(.)$ acts only on $\veceps$} &=& 
     (\m{X}\tr\m{X})^{-1}\m{X}\tr E(\veceps\veceps\tr)
     \m{X}(\m{X}\tr\m{X})^{-1}\\}
  \visible<6->{\remark{Gau\3-Markow} &=& 
     (\m{X}\tr\m{X})^{-1}\m{X}\tr\sigeps^2\m{X}(\m{X}\tr\m{X})^{-1}\\}
  \visible<7->{\remark{def inverse matrix} &=& 
     \sigeps^2(\m{X}\tr\m{X})^{-1} }
\edma
\visible<8->{The variance-covariance matrix depends only on the values of the
exogenous factors!}


}

%###################################################
\frame{\frametitle{Results}
%###################################################

{\small
\bi
\item Ordinary least squares (OLS) estimator:
\maindm{\hatvecbeta =\left(\m{X}\tr\m{X}\right)^{-1}\m{X}\tr\vec{y}}

\pause \item \bfdef{Variance-Covariance matrix} of the estimation errors
(provided the errors are i.i.d.) can be written in terms of the
\bfdef{Hesse matrix $\m{H}$} of the  
objective function SSE:

\maindm{
\begin{array}{rcl}
\m{V}_{\hatvecbeta}
 &=&  E\left( (\hatvecbeta-\vecbeta)(\hatvecbeta-\vecbeta)\tr\right)
 =\sigma^2 \left(\m{X}\tr\m{X}\right)^{-1}= 2\sigma^2 \m{H}^{-1},\\
H_{jk} &=& \left. \ablpartmix{S}{\beta_j}{\beta_k}\right|_{\vecbeta=\hatvecbeta}
=2(\m{X}\tr\m{X})_{jk}
\end{array}
}

\pause \item Variances of estimation errors: $V(\hatbeta_j)=V_{jj}$

\pause \item Correlation of estimation errors: $\text{Corr}(\hatbeta_j, \hatbeta_k)=\frac{V_{jk}}{\sqrt{V_{jj}V_{kk}}}$

\pause \item Distribution of the normalized estimation errors:
$
\frac{\hatbeta_j-\beta_j}{\sqrt{V_{jj}}} \sim N(0,1)
$
\ei
}

}
 
%###################################################
\frame{\frametitle{Estimation of the residual variance}
%###################################################
The above cannot be applied directly since the residual variance $\sigma^2$ is
unknown and must be estimated by the minimum SSE $S(\hatbeta)$: 
\maindm{
\hatsig^2 = \frac{1}{n-J-1}\sum_i \left(y_i-\hat{y}(\vec{x}_i)\right)^2
 = \frac{S(\hatbeta)}{n-J-1}
}

Under the Gau\3-Markow assumptions, this can be expressed as the sum
of squared Gaussians as follows (derivation for the experts):
{\small
\begin{align*}
(n-J-1)\hatsig^2
 &=\left(\hat{\vec{y}}-\vec{y}\right)\tr\left(\hat{\vec{y}}-\vec{y}\right)\\
 &=(\m{X}\hatvecbeta-\vec{y})\tr(\m{X}\hatvecbeta-\vec{y})\\
 &=(\m{X}\hatvecbeta)\tr(\m{X}\hatvecbeta)
 - (\m{X}\hatvecbeta)\tr\vec{y}-\vec{y}\tr(\m{X}\hatvecbeta)
 + \vec{y}\tr\vec{y}
\end{align*}
With following rule for scalar products:
$\vec{a}\tr\vec{b}=\vec{b}\tr\vec{a}$ it follows that the two middle terms
are equal. Replacing $\hatvecbeta=(\m{X}\tr\m{X})^{-1}\m{X}\tr \vec{y}$ we see
that, interestingly, the first term is the negative of each of the
two middle terms resulting in
\bdm
(n-J-1)\hatsig^2
=\vec{y}\tr \left(\m{1}-\m{X}(\m{X}\tr\m{X})^{-1}\m{X}\tr\right)\vec{y}
\edm
}
}

%###################################################
\frame{\frametitle{Estimation of the residual variance (ctned)}
%###################################################

Replace the observed endogeneous data vector $\vec{y}$ by
the model $\vec{y}=\m{X}\vecbeta+\veceps$ {\scriptsize \red{Notice:
    $\vecbeta$ is the
    true and immutable 
  parameter vector $\vecbeta$ }}:

{\small
\begin{align*}
(n-J-1)\hatsig^2
&= (\m{X}\vecbeta+\veceps)\tr
 \left(\m{1}-\m{X}(\m{X}\tr\m{X})^{-1}\m{X}\tr\right)(\m{X}\vecbeta+\veceps)\\
&= \veceps\tr (\m{1}-\m{X}(\m{X}\tr\m{X})^{-1}\m{X}\tr)\veceps\\
& + 2(\m{X}\vecbeta)\tr(\m{1}-\m{X}(\m{X}\tr\m{X})^{-1}\m{X}\tr)\veceps\\
& + \vecbeta\tr\m{X}\tr(\m{1}-\m{X}(\m{X}\tr\m{X})^{-1}\m{X}\tr)\m{X}\vecbeta
\end{align*}

After doing some simplifications, we realize that the second and third
term are each equal to zero, so
\bdm
(n-J-1)\hatsig^2
=\veceps\tr (\m{1}-\m{X}(\m{X}\tr\m{X})^{-1}\m{X}\tr)\veceps.
\edm
With the Gau\3-Markow-assumptions, we have 
% can derive E(\hatsig^2) =\sigma^2 for df=1,2 by direct insertion
% with E(\veceps\tr \m{M} \veceps)=\sigma^2\sum_i M_{ii} 
% (df=2: n from $\m{1}$ and $-2$ from
% $-\m{X}(\m{X}\tr\m{X})^{-1}\m{X}$)
% but the M_{ii} are not identical, so, on the level of distributions, 
% it does not work. But all contributions are at least positive. 
% Approximation?
\bdm
(n-J-1)\hatsig^2
=\sigma^2 \sum_{i=1}^{n-J-1}Z_i^2 \sim \chi^2(n-1-J)
\edm
where $Z_i\sim \text{i.i.d. } N(0,1)$ are standard Gaussians and the
sum of i.i.d. such Gaussians defines the $\chi^2$ distribution with
$n-J-1$ degrees of freedom (df)
}  %small

}



%###################################################
\frame{\frametitle{Results if the variance needs to be estimated}
%###################################################

\bi
\item Estimated variance-covariance matrix:

\maindm{\hat{\m{V}}_{\hatvecbeta}= 2\hatsig^2 \m{H}^{-1}
=\hatsig^2 \left(\m{X}\tr\m{X}\right)^{-1}
}
\vspace{1em}

\pause \item \emph{Normalized} 
estimation errors for unknown variance:
{\small
\begin{align*}
\frac{\hatbeta_j-\beta_j}{\sqrt{\hat{V}_{jj}}} 
 &= \frac{\hatbeta_j-\beta_j}{\sqrt{V_{jj}}} \, \frac{\sigma}{\hatsig}
 = Z \frac{\sigma}{\hatsig}\\
 &= Z \sqrt{\frac{\sigma^2}{\hatsig^2}} 
 = \frac{Z}{\sqrt{\frac{\sum_{i=1}^{n-J-1}Z_i^2}{n-1-J}}}
\end{align*}
}
A random variable with a standard Gaussian in the numerator and, in
the denominator, the
square root of a $\chi^2$ distributed random variable with df degrees
of freedom divided by df, defines a \bfdef{student-t distribution} with
df degrees of freedom:

\maindm{
\frac{\hatbeta_j-\beta_j}{\sqrt{\hat{V}_{jj}}} \sim T(n-1-J)
}
\ei


}
 
%###################################################
\frame{\frametitle{Multivariate distribution function of $\hatvecbeta$}
%###################################################

{\small
The distribution of the errors $\Delta
\hatvecbeta=\hatvecbeta-\vecbeta$ obeys a multivariate normal
distribution: 
\bdm
f_{\hatvecbeta}(\Delta \hatvecbeta)
 \propto \exp\left[-\frac{1}{2} 
 \Delta \hatvecbeta\tr \ \m{V}^{-1}\ \Delta \hatvecbeta\right]
   = \exp\left[-\frac{\Delta \hatvecbeta\tr \ \m{X}\tr\m{X}\ 
 \Delta \hatvecbeta}
  {2\sigeps^2}\right].
\edm

\pause
\mysubsubheading{Relation to the maximum-likelihood-method ($\to$
  Lecture 07:)}

Expand the SSE $S(\vecbeta)$ around $\hatvecbeta$ to second order:
\bdm
S(\vecbeta)-S(\hatvecbeta)
\approx \frac{1}{2}\Delta \hatvecbeta\tr \ \m{H} \ \Delta \hatvecbeta
= \Delta \hatvecbeta\tr \ \m{X}\tr\m{X} \ \Delta \hatvecbeta
\edm
\vspace{1em}

\pause
\hspace{5em} $\Rightarrow \quad$ 
\vspace{-3em}

\maindm{f_{\hatvecbeta}(\Delta \hatvecbeta)
 \propto \exp\left[-\frac{S(\vecbeta)-S(\hatvecbeta)}{2\sigeps^2}\right] 
}

\pause and with the estimated residual variance
$\hatsigeps^2=S(\hatvecbeta)/(n-J-1)$  
\bdm
\hat{f}_{\hatvecbeta}(\vecbeta) \propto \exp \left[
-\frac{(n-J-1)}{2}\left(\frac{S(\vecbeta)}{S(\hatvecbeta)}-1\right)
\right]
\edm
}

}

%###################################################
\frame{\frametitle{Example of correlated errors:
modeling the demand for hotel rooms}
%###################################################

\placebox{0.26}{0.64}
{\figSimple{0.46\textwidth}{figsRegr/hotel_scatterplot_x1x2_eng.png}}


\placebox{0.26}{0.22}
{\figSimple{0.46\textwidth}{figsRegr/hotel_scatterplot_x1y_simple_eng.png}}

\placebox{0.74}{0.22}
{\figSimple{0.46\textwidth}{figsRegr/hotel_scatterplot_x2y_simple_eng.png}}

\placebox{0.74}{0.61}
{\parbox{0.53\textwidth}{\scriptsize
\bi
\item The example of Lecture 02: \\[-1ex]
  \bdm
  y=\beta_0+\beta_1x_1+\beta_2x_2+\epsilon
\edm \\[-1ex]
\pause \item Exogenous factors: 
  $x_0=1$, $x_1$: proxy for quality [\# stars]; $x_2$: price 
  [\euro{}/night].
\pause \item Endogenous variable: booking rate [\%]
\pause \item The demand is positively correlated with both the quality and
  the price
\ei
}}

}

%###################################################
\frame{\frametitle{Residual errors for fitted parameters} 
%###################################################

\fig{0.9\textwidth}{figsRegr/hotel_scatterplot_x2ycondx1_eng.png}
}


%###################################################
\frame{\frametitle{Effect of mis-fit parameters I: small effect 
   if $\bfbeta_1$ and $\bfbeta_2$ have opposite misfits}
%###################################################
\fig{0.9\textwidth}{figsRegr/hotel_scatterplot_x2ycondx1_notCal2_eng.png}
}

%###################################################
\frame{\frametitle{Effect of mis-fit parameters II: small effect 
   if $\bfbeta_1$ and $\bfbeta_2$ have opposite misfits}
%###################################################
\fig{0.9\textwidth}{figsRegr/hotel_scatterplot_x2ycondx1_notCal4_eng.png}
}

%###################################################
\frame{\frametitle{Effect of mis-fit parameters III: large effect 
   if $\bfbeta_1$ and $\bfbeta_2$ have both positive misfits}
%###################################################
\fig{0.9\textwidth}{figsRegr/hotel_scatterplot_x2ycondx1_notCal1_eng.png}
}

%###################################################
\frame{\frametitle{Effect of mis-fit parameters IV: large effect 
   if $\bfbeta_1$ and $\bfbeta_2$ have both negative misfits}
%###################################################
\fig{0.9\textwidth}{figsRegr/hotel_scatterplot_x2ycondx1_notCal3_eng.png}
}


%###################################################
\frame{\frametitle{All this results in a negative correlation\\
    between the estimation errors for $\bfbeta_1$ and $\bfbeta_2$}
%###################################################

\vspace{1em}

\fig{0.90\textwidth}{figsRegr/hotel_f2_hatbeta1_hatbeta2_simple_eng.png}

}

%###################################################
\frame{\frametitle{Special case 1: No exogenous variables}
%###################################################
{\small
\bi
\item Model: $y=\beta_0+\epsilon := \mu+\epsilon$
\pause \item System matrix: $\m{X}=(1,1, ..., 1)\tr$
\pause \item OLS estimator: 
\bdma 
\left(\m{X}\tr\m{X}\right)^{-1}=\frac{1}{n}, \quad 
\m{X}\tr \vec{y}=\sum_iy_i=n\bar{y}, &&\\
\hatbeta_0=\hat{\mu}=\left(\m{X}\tr\m{X}\right)^{-1}\m{X}\tr \vec{y}=\bar{y}
\edma
\pause \item Variance:
  $V_{00}=V(\hat{\mu})=\sigma^2\left(\m{X}\tr\m{X}\right)^{-1}
=\frac{\sigma^2}{n}$, \quad
$\hat{V}_{00}=\frac{\hatsig^2}{n}$
\vspace{1em}

\pause \item Distribution of the estimator
 \red{(if $\epsilon \sim i.i.d N(\mu,\sigma^2)$)}

\bdma
\frac{\hatbeta_0-\beta_0}{\sqrt{V_{00}}}
&=& \frac{\bar{y}-\mu}{\sigma}\ \sqrt{n} \sim N(0,1), \\
\frac{\hatbeta_0-\beta_0}{\sqrt{\hat{V}_{00}}}
&=& \frac{\bar{y}-\mu}{\hatsig}\ \sqrt{n} \sim T(n-1)
\edma
\ei
}

}

%###################################################
\frame{\frametitle{Special case 2: Simple linear regression}
%###################################################
\bi
\item Model (with $x_1=x$): $y=\beta_0+\beta_1x+\epsilon$

\pause \item System matrix: 
\bdm \m{X}=\myMatrixTwo{1 & x_1 \\ \vdots & \vdots \\ 1 & x_n},\quad
\m{X}\tr\m{X}=\myMatrixTwo{n & n\bar{x}\\n\bar{x} & \sum x_i^2}
\edm

\pause \item OLS estimator (with $s_x^2=1/n(\sum x_i^2-n\bar{x})$): 
\bdm
\left(\m{X}\tr\m{X}\right)^{-1}
 =\frac{1}{ns_x^2}
\myMatrixTwo{\frac{\sum x_i^2}{n} & -\bar{x} \\ -\bar{x} & 1}, \quad
\m{X}\tr \vec{y}=\myVector{n\bar{y}\\ \sum x_iy_i} 
\edm
\bdma
\hatbeta_1 &=& \left(-\frac{\bar{x}}{ns_x^2}, \frac{1}{ns_x^2}\right)
\myVector{n\bar{y}\\ \sum x_iy_i}
=\frac{\sum_ix_iy_i-n\bar{x}\bar{y}}{\sum x_i^2-n\bar{x}}
=\frac{s_{xy}}{s_x^2}, \\
\hatbeta_0 &=& \bar{y}-\hatbeta_1 \bar{x}
\edma
\ei
}

%###################################################
\frame{\frametitle{Simple linear regression (ctnd)}
%###################################################

\bi
\item Variance-covariance matrix (assuming w/o loss of generality
$\bar{x}=0$): 
\bdm
\m{V}(\hatvecbeta)=\sigma^2 \left(\m{X}\tr\m{X}\right)^{-1}
=\sigma^2 \myMatrixTwo{\frac{1}{n} & 0 \\ 0 & \frac{1}{ns_x^2}}
\edm

\pause \item Variance of the estimator $\hat{y}(x)$ ($x$ is deterministic):
\bdm
V(\hat{y}(x))=V(\hatbeta_0+\hatbeta_1x)
 = V_{00}+x^2  V_{11} +2xV_{01}
 = \frac{\sigma^2}{n}\left(1+\frac{x^2}{s_x^2}\right)
\edm

\pause \item Distribution of the estimator for $y(x)$:
\bdm
\hat{y}(x)\sim N\big(y(x), V(\hat{y}(x))\big)
\edm
If $\sigma^2$ has to be estimated by $\hatsig^2$, the normalized
estimators for $\beta_0$, $\beta_1$ and $y(x)$ are $\sim T(n-2)$.

\ei
}



%###################################################
\frame{\frametitle{Probability density for $\hat{y}(x)$ for simple 
linear regression}
%###################################################

\vspace{-0.5em}
\fig{0.8\textwidth}{figsRegr/regression3dFig.png}
\vspace{-2em}

\bi
\item If the Gau\3-Markov assumptions apply, the model
  estimation errors $\hat{y}(x)-y(x)$ are
  Gaussian distributed
\item The expectation and variance depends on $x$; the standard error
  is hyperbola-shaped.
\ei

}

\subsection{3.2 Confidence Intervals}

%###################################################
\frame{\frametitle{3.2. Confidence Intervals:\\
 where the
Student-t distribution comes from}
%###################################################

\visible<1>{\placebox{0.489}{0.468}{
  \figSimple{1.02\textwidth}{figsRegr/KI_veranschaulichung_eng1.png}}}

\visible<2>{\placebox{0.48}{0.45}{
  \figSimple{1.0\textwidth}{figsRegr/KI_veranschaulichung_eng2.png}}}

\visible<3>{\placebox{0.502}{0.453}{
  \figSimple{1.05\textwidth}{figsRegr/KI_veranschaulichung_eng3.png}}}

\visible<4>{\placebox{0.502}{0.453}{
  \figSimple{1.05\textwidth}{figsRegr/KI_veranschaulichung_eng4.png}}}


}

%###################################################
\frame{\frametitle{Densities of standard normal 
\textit{vs.} Student-t distribution}
%###################################################

\fig{0.90\textwidth}{figsRegr/f_gaussStudent_eng.png}
}


%###################################################
\frame{\frametitle{Distributions of standard normal 
\textit{vs.} Student-t-distribution}
%###################################################

\fig{0.90\textwidth}{figsRegr/F_gaussStudent_eng.png}
}

%###################################################
\frame{\frametitle{Calculation of the confidence intervals (CI)}
%###################################################

\placebox{0.50}{0.81}{\parbox{1.0\textwidth}{
\maindm{\text{CI}_{\beta_j}^{(\alpha)}: \beta_j \in \left[
 \, \hatbeta_j-\Delta \hatbeta_j,  \hatbeta_j+\Delta \hatbeta_j \,
\right], \quad
\Delta \hatbeta_j =t_{1-\alpha/2}^{(n-J-1)}\hatsig_{\hatbeta_j}.
}}}

\placebox{0.50}{0.70}{\parbox{1.0\textwidth}{
 {\small
 \bi
  \pause \item $t_{1-\alpha/2}$: Quantile (inverse of) the distribution
    function\\[-1ex]
  \pause \item CI ``uncertainty principle'': Higher sensitivity implies
    higher $\alpha$ error.
  \ei
}
}}

\visible<3>{\placebox{0.50}{0.31}
{\figSimple{0.70\textwidth}{figsRegr/f_student_KI_eng.png}}}

\visible<2>{\placebox{0.50}{0.31}
{\figSimple{0.70\textwidth}{figsRegr/F_student_KI_eng.png}}}



}


%###################################################
\frame{\frametitle{Hotel example: CI for the
    appraisal for ``stars'' 
    $\beta_1$\\ \hspace*{0.65\textwidth}(full model)}
 %###################################################
\vspace{-2em}
\begin{tabular}{cc}
\parbox{0.5\textwidth}{
  \visible<1->{\fig{0.48\textwidth}{figsRegr/hotel_f_hatbeta1_eng.png}}
  \vspace{-2em}
  \visible<2->{\fig{0.48\textwidth}{figsRegr/hotel_F_hatbeta1_eng.png}}
}
&
\hspace{1em}
\parbox{0.48\textwidth}{

Model: $y(\vec{x})=\sum_j\beta_jx_j+\epsilon$
\vspace{1em}

Factors:\\[-0.5ex]
{\small $x_0=1$, $x_1$: \#stars, $x_2$: price}
\vspace{1em}

Confidence interval (CI):
\vspace{0.5em}

{\small 
$\beta_1\in \left[\hat{\beta}_1-\Delta \hat{\beta}_1^{(\alpha)},
 \hat{\beta}_1+\Delta \hat{\beta}_1^{(\alpha)}\right]$

\pause $\Delta \hat{\beta}_1^{(\alpha)}=t_{1-\alpha/2}^{(n-3)}
\sqrt{\hat{V}(\hat{\beta}_1)}$

\pause $\hat{V}(\hat{\beta}_1)
  =\hatsigeps^2\left[\left(\m{X}\tr\m{X}\right)^{-1}\right]_{11}$

\pause $\hatsigeps^2=\frac{1}{n-3}\sum\limits_{i=1}^n
 \left(\hat{y}_i-y_i\right)^2$
}
}
\end{tabular}
}




\end{document}


