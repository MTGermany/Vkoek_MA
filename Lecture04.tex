%#############################################
% New pdflatex format,
% only png, jpg (or svg?) images but positioning now ok
% template in ~/tex/inputs/template_folien.tex
% [TODO now specialized to Vkoem_Ma; make general]
%#############################################


%\documentclass[mathserif]{beamer}
\documentclass[mathserif,handout]{beamer}
%\documentclass[mathserif,aspectratio=1610]{beamer}
%\documentclass[mathserif,aspectratio=1610,handout]{beamer}
\input{$HOME/tex/inputs/defsSkript}%$
\input{$HOME/tex/inputs/styleBeamerVkoekMa}%$
\usepackage{graphicx}


%##############################################################

\begin{document}

\section{4. Significance Tests}

%###################################################
\frame{  %title other layout
%###################################################
\placebox{0.50}{0.50}{
  \figSimple{1.70\textwidth}{figsRegr/regression3dFig.png}}

\makePale{0.85}{0.50}{0.55}{1.20}{1.50}

\placebox{0.50}{0.42}{\parbox{0.6\textwidth}{
\bi
\item[4] Significance Tests
\item[4.1] General Four-Step Procedure
\bi
\item[4.1.1] Step 1: Chosing $H_0$: Type I and II errors
\item[4.1.2] Steps 2 and 3: Test statistics
\item[4.1.3] Steps 4: Decision
\item[4.1.4] Step 4a: The $p$-value
\ei
\item[4.2] Dependence on the True Parameter Value: Power Function
\item[4.3] Model Selection Strategies
\item[4.4] Logistic Regression
\ei
}}

\placebox{0.50}{0.90}{\myheading{
 Lecture 04: Classical Inferential Statistics II:}}

\placebox{0.50}{0.82}{\myheading{Significance Tests}}


}

\subsection{4.1 General Four-Step Procedure}

%###################################################
\frame{\frametitle{4. Significance Tests\\
4.1 General Four-Step Procedure}
%###################################################
%}\end{document}
%\benum
%\item test
%\benum
%\item 2nd level
%\benum
%\item 3rd level
%\eenum
%\eenum
%\eenum
%}\end{document}
\benum
\item Formulate a \bfdef{null hypothesis $H_0$} such that their rejection
  gives insight, e.g. $\beta_j=\beta_{j0}$ (point hypothesis) or
  $\beta_j \le \beta_0$ (interval hypothesis): Notice: \emph{One
    cannot confirm $H_0$}
\pause \item Select a \bfdef{test function} or \bfdef{statistics $T$}
\bi
\item whose distribution is known provided the parameters are at the 
\bfdef{margin $H^*_0$ of the null hypothesis} (of course, $H^*_0=H_0$ for a point
null hypothesis) 

\pause
\colAsk{\scriptsize{What if the estimator has a known
    distribution but the variance is unknown?}}
\\[-0.5ex]
\pause
\colAnswer{\scriptsize{Test function in units of the estimated standard deviation}}
\pause \item which has distinct \bfdef{rejection regions $R(\alpha)$} which
  are reached rarely 
  (with a probability $\le \alpha$) if $H_0$ but more often if
  $H_1=\overline{H_0}$ 
\ei

\pause \item Evaluate a realisation $t\sub{data}$ of $T$ from the data

\pause \item Check if $t\sub{data} \in R(\alpha)$. If yes, $H_0$ can be
  rejected at an error probability or \bfdef{significance level
  $\alpha$}. Otherwise, \emph{nothing can be said} \colAsk{(mask
    example with $H_0$: ``mask useless'')}.
\pause \item[4a] Alternatively, calculate the \bfdef{$p$-value} as
the minimum $\alpha$ at which $H_0$ can be rejected.
\eenum
}

%###################################################
\frame{\frametitle{4.1.1 Step 1: Choosing $\mathbf{H_0}$: 
Type I and II errors}
%###################################################

\visible<1>{\placebox{0.5}{0.69}{
 \figSimple{0.59\textwidth}{figsRegr/fehlerErsterZweiterArt1_eng.png}}}

\visible<2>{\placebox{0.5}{0.69}{
 \figSimple{0.59\textwidth}{figsRegr/fehlerErsterZweiterArt2_eng.png}}}

\visible<3>{\placebox{0.5}{0.69}{
 \figSimple{0.59\textwidth}{figsRegr/fehlerErsterZweiterArt3_eng.png}}}

\visible<4->{\placebox{0.5}{0.69}{
 \figSimple{0.59\textwidth}{figsRegr/fehlerErsterZweiterArt4_eng.png}}}

\placebox{0.5}{0.25}{\parbox{\textwidth}{
{\small
\bi
\visible<1->{\item A significance test reduces reality to a ``binary
  in-binary out'' setting.}
\visible<2->{There are two combinations corresponding to a
  correct test result}
\visible<3->{\item We can control the \bfdef{type I or $\alpha$-error}
  probability 
  $P(\text{$H_0$ rejected}|H_0) \le \alpha$ in \bfdef{significance tests}}
\visible<4->{\item Since the \bfdef{type II or $\beta$-error} probability 
  $P(\text{$H_0$ not rejected}|\overline{H_0})$ is unknown, the more serious
error type should be the $\alpha$ error}
\visible<5->{\item \maintextbox{0.85\textwidth}{Fundamental problem: I want $P(H_0|\text{rejected})$ and
$P(H_0|\overline{\text{rejected}})$ while I get control over 
$P(\text{rejected}|H_0)\le P(\text{rejected}|H_0^*)$  $\Rightarrow$
\bfdef{Bayesian statistics}} }
\ei
}}}

}


\subsubsection{4.1.2 Test statistics}
%###################################################
\frame{\frametitle{4.1.2 Steps 2 and 3: Test statistics I}
%###################################################

{\small
\bi
\item (i) Testing  \red{parameters} such as $H_0$: $\beta_j=\beta_{j0}$ or
  $\beta_j\ge \beta_{j0}$ or  $\beta_j\le \beta_{j0}$:

  The test
  function is the estimated deviation from $H_0^*$ in
  units of the estimated error standard deviation. It is
  \bfdef{student-t} distributed with \#dataPoints- \#parameters
  \bfdef{degrees of 
  freedom (df)}:  
\bdm 
T =\frac{\hatbeta_j-\beta_{j0}}{\sqrt{\hat{V}_{jj}}} \sim T(n-1-J)
\edm
\pause \item (ii) Testing \red{functions of parameters} such as
  $H_0$: $\beta_1/\beta_2=2$, $\le 2$ or $\ge 2$: Transform into a linear
  combination. Then, the normalized estimated deviation is student-t
  distributed under $H_0^*$. Here, at $H_0^*$, the linear combination is
  $b=\beta_1-2\beta_2=0$: 
\bdma
\hat{b} &=& \hatbeta_1-2\hatbeta_2, \\
\hat{V}(\hat{b}) &=& \hat{V}_{11}+4\hat{V}_{22}-4\hat{V}_{12}, \\
T &=& \frac{\hat{b}}{\sqrt{\hat{V}(\hat{b})}} \sim T(n-1-J)
\edma
\ei
}

}

%###################################################
\frame{\frametitle{Test statistics II}
%###################################################

\bi
\item (iii) Testing the \red{correlation coefficient} in an $xy$ scatter plot:
\bdm
\hat{\rho}=\frac{s_{xy}}{s_xs_y}, \quad H_0: \rho=0, \quad
T=\frac{\hat{\rho}}{\sqrt{1-\hat{\rho}^2}}\sqrt{n-2} \sim T(n-2)
\edm

\pause \footnotesize{\colAnswer{
\emph{Derivation:} $\rho =0$ if, and only if, 
in a simple linear regression
$y=\beta_0+\beta_1x+\epsilon$, the slope parameter $\beta_1=0$, so
test for $\beta_1=0$: Under $H_0$, the test statistics
\bdm
T=\hatbeta_1/\sqrt{\hat{V}_{11}}=\frac{s_{xy}}{\hat{\sigma} \ s_x}\sqrt{n}
 \sim T(n-2)
\edm
Now insert $\hat{\sigma}$ which can, in the simple-regression case, be
explicitely calculated:
 $\hat{\sigma}^2=n (s_y^2 -s_{xy}^2/s_x^2)/(n-2)$
}}

\vspace{1em}

\pause \item (iv) Test for the \red{residual variance}, $H_0$: $\sigma^2=\sigma_0^2$, 
$\sigma^2 \ge \sigma_0^2$, and $\sigma^2 \le \sigma_0^2$:
\bdm
T=\frac{\hatsig^2}{\sigma_0^2} \ (n-1-J) \sim \chi^2(n-1-J)
\edm
The one-parameter \bfdef{chi-squared distribution with $\bfmath{m}$ degrees of
  freedom} $\chi^2(m)=\sum_{i=1}^mZ_i^2$ is the
sum of squares of i.i.d. Gaussians. \emph{Its density is not symmetric,
  so we need to calculate both the $\alpha$ and $1-\alpha$ quantiles} 

\ei
}

%###################################################
\frame{\frametitle{Test statistics III}
%###################################################

{\small
\bi

\item (v) Tests of \red{simultaneous point null hypotheses}, e.g., $H_0$: 
$(\beta_1=0)$ AND $(\beta_2=2)$ using the
\bfdef{Fisher-F test}:
\bdm
T=\frac{ (S_0-S)/(M-M_0)}{S/(n-M)} \sim F(M-M_0,n-M)
\edm
\bi
\pause \item $S$: SSE of the estimated full model with $M=J+1$ parameters
\pause \item $S_0$: SSE of the estimated restrained model under $H_0$ with
  $M_0$ free parameters
\ei

\pause \item The \bfdef{Fisher-F} distribution is essentially the ratio of two
  independent $\chi^2$ distributed random variables,
\bdm 
F(n,d)=\frac{\chi_n^2/n}{\chi_d^2/d},
\edm
with $n$ numerator and $d$ denominator degrees of freedom

\pause \itemAsk  Argue that always $S_0\ge S$ 

\ei
}
}



%###################################################
\frame{\frametitle{Equivalence of the F and T-tests for one parameter}
%###################################################
\vspace{1em}

{\small
With $M-M_0=1$, the F-test is equivalent to a parameter test for the
  parameter $j$ in question:
\bi
\item Parameter test:
$
T=\frac{\hatbeta_j-\beta_{j0}}{\sqrt{\hat{V}(\hatbeta_j)}}\sim T(n-1-J)
$

\item F-test:
$
T=(n-J-1)\frac{S_0-S}{S} \sim F(1,n-1-J)
$
\pause \itemAsk \colAsk{\scriptsize{Regarding the rhs., show following
    general relation between the 
  student-t and the F(1,d) distributions: 
$
F\sim F(1,d) \ \text{and} \ T \sim T(d) \Rightarrow F=T^2
$
}}

\pause \itemAnswer \colAnswer{\tiny{By definition, Fisher's F is a
    ratio of $\chi^2$ distributions. Furthermore, squares of
    standardnormal random variables $Z$ are $\chi_1^2$ distributed:
\bdm
F(1,d)=\chi_1^2/(\chi_d^2/d)=Z^2/(\chi_d^2/d)
\edm
where $Z\sim N(0,1)$ and $\chi_d^2$ and $Z$ are independent from each
other. 
The definition of the student-t distribution is 
$T(d)=Z/\sqrt{\chi_d^2/d}$, so $F(1,d)=T_d^2$.}}

\pause \item One can show (difficult!) that following is exactly valid for
  the lhs.:
 \bdm
 (n-J-1)\frac{S_0-S}{S}
=\frac{(\hatbeta_j-\beta_{j0})^2}{\hat{V}(\hatbeta_j)}
=\frac{(\hatbeta_j-\beta_{j0})^2}{\hat{V}_{jj}}
\edm
where $S_0$ is the (minimum) SSE for the
calibrated restrained model


\ei
}

}

\subsubsection{4.1.3 Step 4: Decision}

%###################################################
\frame{\frametitle{4.1.3 Step 4: Decision}
%###################################################
\bi
\item The decision is based on the \emph{rejection region}:
\maintextbox{0.7\textwidth}{The \bfdef{rejection region
    $R^{(H_0)}(\alpha)$} contains the fraction $\alpha$ of all
  realisations $t$ of the test statistics $T$ which, under $H_0^*$,
  are most distant 
  from $H_0$}

\pause \item Decision: 
\maintextbox{0.80\textwidth}{$H_0$ is rejected at significance level
  $\alpha$ if  $t\sub{data}\in
  R^{(H_0)}(\alpha)$} 

\pause \item A good test statistics allows for a clear definition of what is
  meant by ``distance to $H_0$'' and brings, for a given $\alpha$, 
 the boundary of the rejection
  region as close to $H_0^*$ as possible

\pause \item In contrast to $T$ and the realisation $t\sub{data}$ which only depends
  on $H_0^*$ and therefore is the same for point and interval
  hypotheses of the same kind, the rejection region is different for
  the different comparison operators $=$, $\ge$, $\le$
\ei

}

%###################################################
\frame{\frametitle{1. Rejection region for $\vec{H}_0$: ``$<$''
or ``$\le$'' (interval hypothesis)}
%###################################################

\placebox{0.5}{0.58}{
 \figSimple{0.7\textwidth}{figsRegr/annahmeAblehn_einZweiseit1_eng.png}}

\pause
\placebox{0.5}{0.16}{\parbox{\textwidth}{
\bi
 \item $H_0$ is rejected on the level $\alpha$ if
\maindm{
t\sub{data}>t_{1-\alpha}
}
\ei
}}

}


%###################################################
\frame{\frametitle{2. Rejection region for $\vec{H}_0$: ``$>$''
or ``$\ge$'' (interval hypothesis)}
%###################################################

\placebox{0.5}{0.58}{
 \figSimple{0.7\textwidth}{figsRegr/annahmeAblehn_einZweiseit2_eng.png}}

\pause \placebox{0.5}{0.16}{\parbox{\textwidth}{
\bi
 \item $H_0$ is rejected on the level $\alpha$ if
\maindm{t\sub{data}<t_{\alpha}=-t_{1-\alpha}}
\pause \item The equality sign is only valid for symmetric test
  statistics
\ei
}}

}

%###################################################
\frame{\frametitle{3. Rejection region for $\vec{H}_0$: ``='' (point hypothesis)}
%###################################################

\placebox{0.5}{0.65}{
 \figSimple{0.6\textwidth}{figsRegr/annahmeAblehn_einZweiseit3_eng.png}}

\placebox{0.5}{0.21}{\parbox{\textwidth}{
{\small
\bi

 \item For symmetric test statistics, 
$H_0$ is rejected on the level $\alpha$ if
\maindm{|t\sub{data}|>t_{1-\alpha/2}}
\pause \item If the distribution is not symmetric (as the $\chi^2$
  distribution for the variance test), the definition of what is
  ``most distant'' is not unique. For simplicity, one assumes equal
  statistical weights to both sides:
\bdm
\text{rejected} \quad \Leftrightarrow 
(t\sub{data}<t_{\alpha/2}) \cup (t\sub{data}>t_{1-\alpha/2})
\edm
\ei
}
}}

}

%###################################################
\frame{\frametitle{Example: modeling the demand for hotel rooms}
%###################################################

{\small 
The already well-known example for $y(\vec{x})$: hotel room occupancy [\%] 
\bdm
y=\beta_0x_0+\beta_1x_1+\beta_2x_2+\epsilon
\edm
where $x_0=1$, $x_1$: proxy for quality [\# stars]; $x_2$: price
  [\euro{}/night], 
\bdm
\hatbeta_0=25.5, \quad
\hatbeta_1=38.2, \quad
\hatbeta_2=-0.952
\edm
and
\bdm
\hat{\m{V}}=\myMatrixThree{
28.0 &	-6.40 &	-0.119\\
-6.40 & 26.0 & -0.941\\
-0.119 & -0.941 & 0.0397}
\edm

\bi
\pause \itemAsk \colAsk{Formulate and test the null hypothesis at
  $\alpha=\unit[5]{\%}$ that the stars
  do not matter}
\pause \itemAnswer \scriptsize{\colAnswer{$H_{01}: \beta_1=0$, point t-test
    with $T=\hatbeta_1/\sqrt{\hat{V}_{11}} \sim T(12-3)$, i.e. df=9
    degrees of freedom, $t\sub{data}=7.49$,
    $t^{(9)}_{0.975}=2.26<|t\sub{data}|$ $\Rightarrow$ $H_0$ rejected,
    stars matter}}
\pause \itemAsk \colAsk{Do people favour more stars (at $\alpha=\unit[5]{\%}$)?}  
\pause \itemAnswer \scriptsize{\colAnswer{$H_{02}: \beta_1<=0$ (use as $H_0$
    what you want to reject!), interval
     test with same $T$ and $t\sub{data}$ as above,
     $t^{(9)}_{0.95}=1.83<t\sub{data} \Rightarrow H_{02}
     \text{ rejected}$, more stars are better}}
\ei
}

}

%###################################################
\frame{\frametitle{Example: modeling the demand for hotel rooms (ctned)}
%###################################################

\bi
\itemAsk \colAsk{\small{Does each \euro{} more per night decrease the
    occupancy 
  by at most \unit[1]{\%}?}}

\pause \itemAnswer \scriptsize{\colAnswer{$H_{03}: \beta_2<-1$ ($H_{03}$ is the
    complement event!),
    $t\sub{data}=(\hat{\beta}_2+1)/\sqrt{\hat{V}_{22}}=0.24
    \stackrel{!}{>} t^{(9)}_{0.95}=1.83$ $\Rightarrow$ $H_{03}$
    not rejected\\[0.5ex]
$\Rightarrow$ the hotel manager might risk losing more than one
    percent point of customers
}}


\pause \itemAsk {\small \colAsk{Is it worth renovating my hotel thereby gaining one
  star so that\\[0.5ex] I can ask for \unit[30]{\euro{}} more per
  night without  losing guests?}}

\pause \itemAnswer \scriptsize{\colAnswer{Again, define the complement event as 
$H_{04}: \beta_1\le -30\beta_2$ or $\gamma=\beta_1+30\beta_2\le 0$
\bdma
\hat{\gamma} &=& \hatbeta_1+30\hatbeta_2=9.63,\\
\hat{V}(\hat{\gamma}) &=&\hat{V}_{11}+900 \hat{V}_{22}+2*1*30\hat{V}_{12}=5.27
\edma
\pause So, $t\sub{data}=\hat{\gamma}/\sqrt{\hat{V}(\hat{\gamma})}=4.20>
t^{(9)}_{0.95}=1.83$
$\Rightarrow \ H_{04}$ rejected at \unit[5]{\%}  
$\Rightarrow$ 
the risk of losing customers is less than \unit[5]{\%}}}

\pause \itemAsk {\small \colAsk{Can it be simultaneously true that
    $\beta_1=30$ and $\beta_2=-1$?}}

\pause \itemAnswer \scriptsize{\colAnswer{Full model: 
$\hatvecbeta=(25.5, 38.2, -0.952)\tr$, $S(\hatvecbeta)=498.2$; \\
Reduced model with fixed $\beta_1=30$, $\beta_2=1$ leading to $\hatbeta_0=49.0$:
$\hatvecbeta_r=(49.0, 30, -1)\tr$, $S_0=S(\hatvecbeta_r)=1808$;
\pause
$M-M_0=\unit[2]{df}$,
$n-M=\unit[9]{df}$, $T\sim F(2,9)$,
$t\sub{data}=9/2 \ (S_0-S)/S = 11.8>f^{(2.9)}_{0.95}=4.26$ $\Rightarrow$
$H_0$ rejected}} 

\ei

}



\subsubsection{4.1.4 The $p$-value}

%###################################################
\frame{\frametitle{4.1.4 The $\bfmath{p}$-value}
%###################################################
\bi
\item Obviously, it is not very efficient to test $H_0$ for a fixed
significance level $\alpha$ (one does not know \emph{how significant}
the result really is)
\pause \item Instead, one would like to know the \emph{minimum}
$\alpha$ for rejection (notice the \emph{statistical
  reliability-sensitivity uncertainty 
  relation)}  or the \bfdef{$p$-value}. 
\pause \item The most general definition is:
\maindm{p=\text{Prob}(T\in E\sub{data}|H_0^*))} 
where the \emph{extreme region} $E\sub{data}$ contains all
realisations of $T$ that are
further away from $H_0$ than $t\sub{data}$. Hence, $t\sub{data}$ lies
on the boundary of $E\sub{data}$
\pause \colAsk{\small{Relation to the rejection region?}}
\pause \colAnswer{\small{$p$ is defined such that $E\sub{data}=R(p)$}}

\vspace{1ex}

\bi
\pause \item $p\ge\unit[5]{\%}$: not significant (no star at the value for
  $\beta$, sometimes a ``+'' if between \unit[5]{\%} and
\unit[10]{\%}, e.g., $\beta_1=4.2^+$)
\pause \item $p<\unit[5]{\%}$: significant (one star, e.g., $\beta_1=4.2^*$)
\pause \item $p<\unit[1]{\%}$: very significant (two star, $\beta_1=4.2^{**}$)
\item $p<0.001$: highly significant (three stars, $\beta_1=4.2^{***}$)
\ei
\ei
}

%###################################################
\frame{\frametitle{Calculating $\bfmath{p}$ for some basic tests}
%###################################################

\visible<1->{\placebox{0.84}{0.75}{
 \figSimple{0.30\textwidth}{figsRegr/p_intervalLeft_eng.png}}}

\visible<2->{\placebox{0.84}{0.47}{
 \figSimple{0.30\textwidth}{figsRegr/p_intervalRight_eng.png}}}

\visible<3->{\placebox{0.84}{0.12}{
 \figSimple{0.30\textwidth}{figsRegr/p_point_eng.png}}}

\placebox{0.34}{0.42}{\parbox{0.65\textwidth}{
\bi
\item Interval test $H_0: \beta \le \beta_0$ or $\beta < \beta_0$
\vspace{-1em}
 \bdm
p=P(T>t\sub{data}|\beta=\beta_0)
=1-F_T(t\sub{data})
\edm\\[2em]
\pause \item Interval test $H_0: \beta \ge \beta_0$ or $\beta > \beta_0$
\vspace{-1em}
\bdm
p=P(T<t\sub{data}|\beta=\beta_0)=F_T(t\sub{data})
\edm\\[2em]
\pause \item Point test $H_0: \beta=\beta_0$ (symmetry of $f_T$
assumed at the 3$^{\text{rd}}$ equality sign)
\vspace{-1em}
\bdma
p &=& P\big( (T>|t\sub{data}|) \cup (T<-|t\sub{data}|)\big)\\
 &=& \left(1-F_T(|t\sub{data}|)\right)+F_T(-|t\sub{data}|)\\
 &=& 1-F_T(|t\sub{data}|)+1-F_T(|t\sub{data}|)\\
 &=& 2(1-F_T(|t\sub{data}|))
\edma
\ei
}}

}

%###################################################
\frame{\frametitle{$\bfmath{p}$-values for the null hypotheses of the
    hotel example}
%###################################################

\vspace{-0.5em}
{\small
\bdm
y=\beta_0x_0+\beta_1x_1+\beta_2x_2+\epsilon
\edm
where $x_0=1$, $x_1$: proxy for quality [\# stars]; $x_2$: price

\bi
\item $H_{01}$ ``stars do not matter'': 
point hypothesis $\beta_1=0$
  \\ $t\sub{data}=7.49, \  
 p= 2(1-F_T^{(9)})(|t\sub{data}|)=3.7E-5^{***}$
\pause \item $H_{02}$ ``more stars are better'':
interval hypothesis $\beta_1<0$ \\ $t\sub{data}=7.49,\
p= 1-F_T^{(9)}(t\sub{data})=1.9E-5^{***}$
\pause \item $H_{03}$ ``$\Delta$ occupancy $\le \unit[-1]{\%}$ 
per addtl \euro{}'':
interval hypothesis $\beta_2<-1$ \\ $t\sub{data}=0.24,\
p=1-F_T^{(9)}(t\sub{data})=\unit[40]{\%} $
\pause \item $H_{04}$ One star more is worth $\ge$ \unit[30]{\euro{}}'':\\
 function interval hypothesis
  $\gamma=\beta_1+30\beta_2<0$\\ $t\sub{data}=4.20,\ 
p= 1-F_T^{(9)}(t\sub{data})=\unit[0.12]{\%}^{**}$
\pause \item $H_{05}$ ``star and price sensitivity simultaneously given'':\\
compound point hypothesis $(\beta_1=30) \cap
  (\beta_2=-1)$\\ $t\sub{data}=11.8,\ 
p=1-F_F^{(2,9)}(t\sub{data})=\unit[0.30]{\%}^{**}$
\ei
}

}


%###################################################
\frame{\frametitle{Visualization}
%###################################################
\vspace{-0.5em}
\fig{0.72\textwidth}{figsRegr/hotel_f2_hatbeta1_hatbeta2_eng.png}
\vspace{-0.8em}

{\scriptsize
\bi
\pause\item Turquoise lines: boundaries of the $\alpha=\unit[5]{\%}$-CIs of $\beta_1$ and $\beta_2$
\pause \item Black line: boundary of simple interval
null hypothesis $H_{03}: \beta_2\le -1$ ($t$-test)

\pause \item Red boxes: boundary of the function intervall hypothesis
$H_{04}: \gamma=\beta_1+30\beta_2<0$ ($t$-test)

\pause \item Black symbols: simultaneous point hypotheses ($F$-test)\\
$\bullet$: \ $H_{05}: (\beta_1=30) \cap \beta_2=-1)$,
\quad
$\triangle$: \ $H_{06}: (\beta_1=30) \cap (\beta_2=-0.6)$.
\ei
}
}

\subsection{4.2 Dependence on the True Parameter Value}

%###################################################
\frame{\frametitle{4.2 Dependence on the True Parameter Value}
%###################################################
All statistical tests, including the $p$-values, are based on some
\emph{null hypothesis} which is supposed to be \emph{marginally}
fulfilled, $\beta=\beta_0 \in H_0^*$.  What if the true parameter
values take on other values? 
\bi
\pause\item Since regression parameters are continuous, the probability
  $P(H_0^*)=0$ exactly, so the tests and $p$-values \emph{do not
  reflect reality}
\pause\item What happens for other values $\beta \notin H_0^*$? This is
  quantified by following conditional probability called
  \bfdef{statistical power
    function}: 
\maindm{\pi(\beta)=\text{Pr}(\text{test rejected} | \beta)}
\pause\item If $\beta\notin H_0$, then $\pi(\beta)$ indicates the
  \bfdef{statistical power} or \bfdef{specificity} of a test and 
 $1-\pi(\beta)$ its probability for a type-II error
\pause\item If $\beta\in H_0$, then $\pi(\beta)$ is the type-I ($\alpha$)
  error and $1-\pi(\beta)$ the \bfdef{sensitivity} of a test
\pause\item By definition, $\pi(\beta_0)=\alpha$
\ei
}

%###################################################
\frame{\frametitle{Calculating the statistical power function}
%###################################################

\bi
\item If $\beta\neq \beta_0 \in H_0^*$, then the usual test function, e.g.,
$(\hatbeta_j-\beta_{j0})/\sqrt{\hat{V}_{jj}}$ does \emph{no longer} obey
a standard statistical distribution such as standardnormal or
student-t \\[2em]
\pause \item However, $T=(\hatbeta_j-\beta_j)/\sqrt{\hat{V}_{jj}}$ does:
\bdm
T=\frac{\hatbeta_j-\beta_j}{\sqrt{\hat{V}_{jj}}}
 = \frac{\hatbeta_j-\beta_{j0}}{\sqrt{\hat{V}_{jj}}}
 +\frac{\beta_{j0}-\beta_j}{\sqrt{\hat{V}_{jj}}}
 = \frac{\hatbeta_j-\beta_{j0}}{\sqrt{\hat{V}_{jj}}}-\Delta T
\edm \\[2em]
\pause \item $\Rightarrow$ The independent variable of the power
function is the standardized difference 
$\Delta T=(\beta_j-\beta_{j0})/\sqrt{\hat{V}_{jj}}$


\ei

}

%###################################################
\frame{\frametitle{Example I: Interval test for $<$ and $\le$}
%###################################################
\vspace{-1em}

\bdma
\pi^{\le}(\Delta T)
 & \stackrel{\text{def rejection}}{=} &
   P\left(\frac{\hatbeta_j-\beta_{j0}}{\sqrt{\hat{V}_{jj}}}
    >t_{1-\alpha}\right)\\
\visible<2->{& \stackrel{\text{def $\Delta T$}}{=} & P(T+\Delta
  T>t_{1-\alpha}) \\} 
\visible<3->{ &=& P(T>-\Delta T+t_{1-\alpha}) \\}
\visible<4->{&=& 1-P(T<-\Delta T+t_{1-\alpha})\\}
\visible<5->{ & \stackrel{\text{symm.}}{=} & P(T<\Delta T-t_{1-\alpha})\\}
\visible<6->{ & \stackrel{\text{def distr.}}{=} & 
\uu{F_T(\Delta T-t_{1-\alpha})}}
\edma

\bi
\visible<7->{ \itemAsk \colAsk{\small{Test this expression by calculating
    $\pi^{\le}(0)$ and $\pi'^{\le}(0)$}}}
\visible<8->{\itemAnswer \colAnswer{\scriptsize{Just insert $\Delta T=0$:
\bdma
\pi^{\le}(0) &=& F_T(-t_{1-\alpha})\\
  &=& F_T(t_{\alpha})\\
  &\stackrel{\text{def quantile}}{=} & \alpha \ \OK \\
\pi'^{\le}(0) &=& f_T(-t_{1-\alpha})>0 \ \OK
\edma
}}}

\ei
}

%###################################################
\frame{\frametitle{Type I and II errors for ``$<$'' or
	``$\le$''-tests as a function\\of the true value relative to
    $H_0$, known variance} 
%###################################################
\placebox{0.5}{0.55}{
  \figSimple{0.90\textwidth}{figsRegr/fehler12art_leGauss_eng.png}}

\placebox{0.5}{0.15}{\parbox{0.9\textwidth}{
{\small
\bi
\item The maximum type-I error probability of $\alpha$ 
occurs if $\beta=\beta_0$, i.e., at the boundary of $H_0$.
\pause \item The maximum type-II error probability of $1-\alpha$
occurs if $\beta$ is just outside of $H_0$.
\ei
}}}

}

%###################################################
\frame{\frametitle{The same for unknown variance, 
df=2 degrees of freedom}
%###################################################

\placebox{0.5}{0.55}{
  \figSimple{0.90\textwidth}{figsRegr/fehler12art_leStudent_2FG_eng.png}}

\placebox{0.5}{0.25}{\parbox{0.9\textwidth}{
{\small
\bi
\item The increase with $\Delta T$ is steeper but $\pi(0)=\alpha$ is
  unchanged 
\ei
}}}

}


%###################################################
\frame{\frametitle{Example II: Interval test for for $>$ and $\ge$}
%###################################################

\bdma
\pi^{\ge}(\Delta T)
 & \stackrel{\text{def rejection}}{=} &
  P\left(\frac{\hatbeta_j-\beta_{j0}}{\sqrt{\hat{V}_{jj}}}
    <t_{\alpha}\right)\\
 & \stackrel{\text{def $\Delta T$}}{=} & P(T+\Delta T<t_{\alpha}) \\
 &=& P(T<-\Delta T+t_{\alpha}) \\
 & \stackrel{\text{def distr.}}{=} & \uu{F_T(t_{\alpha}-\Delta T)}
\edma

\bi
\itemAsk \colAsk{\small{Test this expression by calculating
    $\pi^{\ge}(0)$ and $\pi'^{\ge}(0)$}}
\itemAnswer \colAnswer{\scriptsize{Just insert $\Delta T=0$:
\bdma
\pi^{\ge}(0) &\stackrel{\text{def quantile}}{=} & \alpha \ \OK \\
\pi'^{\ge}(0) &=& -f_T(0)<0 \ \OK
\edma
}}

\ei
}

%###################################################
\frame{\frametitle{Type I and II errors for ``$>$'' or
	``$\ge$''-tests, known variance}
%###################################################
\vspace{0em}
\fig{0.9\textwidth}{figsRegr/fehler12art_geGauss_eng.png}
\vspace{-1em}
\bi
\item Again, the maximum type I and II error probabilities of
  $\alpha$ and $1-\alpha$, respectively, are obtained if
  the true parameter(s) are at the boundary / very near outside of $H_0$.
\pause \item The maximum type-I error probability is also known as
significance level.
\ei
}

%###################################################
\frame{\frametitle{The same for unknown variance, 
df=2 degrees of freedom}
%###################################################
\vspace{-4em}
\fig{0.9\textwidth}{figsRegr/fehler12art_geStudent_2FG_eng.png}
}


%###################################################
\frame{\frametitle{Example III: Point test for ``=''}
%###################################################
\bdma
\pi\sup{\,eq}(\Delta T) 
 & \stackrel{\text{def rejection}}{=} & 
 P\left(\left|\frac{\hatbeta_j-\beta_{j0}}{\hatsig_{\hatbeta_j}}\right|
      > t_{1-\alpha/2}\right) \\
\visible<2->{
& \stackrel{\text{def $\Delta T$}}{=} & P(|T+\Delta T| > t_{1-\alpha/2}) \\}
\visible<3->{
&=& P(T+\Delta T > t_{1-\alpha/2}) + P(T+\Delta T <-t_{1-\alpha/2})\\}
\visible<4->{
&=& 1-P(T+\Delta T \le t_{1-\alpha/2})+P(T+\Delta T <-t_{1-\alpha/2})\\}
\visible<5->{
& \stackrel{\text{def distr.}}{=} & 
 1-F_T(t_{1-\alpha/2}-\Delta T) + F_T(-t_{1-\alpha/2}-\Delta T)\\}
\visible<6->{
& \stackrel{\text{symm.}}{=} & 
 \uu{2-F_T(t_{1-\alpha/2}-\Delta T) - F_T(t_{1-\alpha/2}+\Delta T)} }
\edma


\visible<7->{
\bi
\itemAsk \colAsk{\small{Test this expression by calculating
    $\pi^{\le}(0)$}}
\itemAnswer \colAnswer{\scriptsize{Just insert $\Delta T=0$:
\bdm
\pi\sup{\,eq}(0) = 2-(1-\alpha/2)-(1-\alpha/2)=\alpha \ \OK
\edm
}}
}

\ei
}

%###################################################
\frame{\frametitle{Type I and II errors for two-sided (point-)tests \\
	(unkown variance, df=2)}
%###################################################
\vspace{0em}
\fig{0.9\textwidth}{figsRegr/fehler12art_eqStudent_2FG_eng.png}
\vspace{-1em}
\bi
\item Since $H_0$ is a point set here, the type-I error probability is always
  given by $\alpha$ (``significance level'')
\ei
}





\subsection{4.3 Model Selection Strategies}

%###################################################
\frame{\frametitle{4.3 Model Selection Strategies\\
Problem Statement}
%###################################################

\placebox{0.50}{0.45}
{\figSimple{0.60\textwidth}{figsRegr/elephantRaisedTrunkPale.jpg}}

\placebox{0.50}{0.45}
{\parbox{0.80\textwidth}{
\bi
\item With every additional parameter, the fit quality in terms of the
  SSE becomes better (\bfAsk{why?})\\[1em]
\pause \item However, the risk of overfitting increases. In the words of John
  Neumann: \textit{With four parameters I can fit an elephant, and
    with five I can make him wiggle [its] trunk.}\\[1em]
\pause \item Overfitted models do not validate and can make neither
  statements nor predictions.\\[1em]
\pause \item \red{$\Rightarrow$ we need selection criteria taking care of
  overfitting!}
\ei
}}

}

%###################################################
\frame{\frametitle{Model selection: some standard criteria}
%###################################################
\bi
\item \bfdef{(1) Adjusted $R^2$:}
\bdm
\bar R^2 = 1 - \, \frac{n-1}{n-J-1}\,\left(1-R^2\right), \quad
R^2=1-\frac{S}{S_0}, 
\edm
\vspace{-1em}
{\small
\bdm
S =\text{SSE(calibr. full model)}, \quad S_0=\text{SSE(calibr. constant-only model)}.
\edm
}
\vspace{0.1em}

\pause \item \bfdef{(2) Akaike information criterion AIC:}
\begin{equation*}
  \text{AIC} = \ln \hat\sigma\sub{descr}^2 + J\, \frac{2}{n}, 
\end{equation*}

\pause \item \bfdef{(3) Bayes' Information criterion BIC:}
\begin{equation*}
   \text{BIC} = \ln \hat\sigma\sub{descr}^2 + J\, \frac{\ln n}{n} \,.
\end{equation*}
\ei
\visible<3->{Notice that the descriptive
  $\hat{\sigma}\sub{descr}^2=S/n$ instead of 
  the unbiased 
$\hat{\sigma}^2=S/(n-1-J)$ are assumed when defining AIC and BIC.
}

}

%###################################################
\frame{\frametitle{Model selection: Strategy {\`a} la ``Occam's Razor''}
{\small
\begin{itemize}
\item Identify $J$ possibly relevant exogenous factors (the constant
  is always included) and calculate
  $\bar{R}^2$, AIC, or BIC for all $2^J$ combinations of these
      factors (a given factor is either contained or not) by
      \emph{brute force}). 
\pause \item The best model is that maximizing $\bar{R}^2$ or minimizing AIC
  or BIC. 
\pause \item Since AIC and also $\bar{R}^2$ penalize complex models (with
  many parameters) too little, the BIC is usually the best bet.
\pause \item Besides the \emph{brute-force} approach, there are two
  faster strategies that may not find the
  ``best'' model (BIC etc are not transitive)
\bi
\item \bfdef{Top-down approach}: Start with all the $J$
  factors. In each round, eliminate a single factor such that the
  reduced model has the highest increase in $\bar{R}^2$ / decrease in
  AIC or BIC. Stop if there is no further improvement.
\item \bfdef{Bottom-up approach}: Start with the constant-only model
  $y=\beta_0$ and successively add factors until there is no further
  improvement.
\ei
\item Standard statistics packages contain all of these strategies. 
\end{itemize}
}
}

\subsection{4.4. Logistic regression}

%###################################################
\frame{\frametitle{4.4. Logistic regression}
%###################################################


{\small
\bi
\item Normal linear models of the form $Y=\vecbeta\tr\vec{x}+\epsilon$
require the endogenous variable to be continuous (discuss!)
\pause \item Using 
model chaining with an unobservable intermediate continuous 
  variable $Y^*$ allows one to model binary outcomes:
 \maindm{
  Y(\vec{x})= \twoCases{1}{Y^*(\vec{x})> 0}{0}{\text{otherwise,}}
  Y^*(\vec{x})= \hat{y}^*(\vec{x})+\epsilon=\vec{\beta}\tr \vec{x}+\epsilon
}
where $\epsilon$ obeys the \bfdef{logistic distribution} with
$F_{\epsilon}(x)=e^x/(e^x+1)$

\pause \item Probability $P_1$ for the outcome $Y=1$:
\bdm
 P_1 = P(Y^*(\vec{x})> 0)=F_{\epsilon}(\vecbeta\tr \vec{x})
= \frac{e^{\vecbeta\tr \vec{x}}}{e^{\vecbeta\tr \vec{x}}+1}
\edm

\pause \item Formally, this is a normal linear regression model for
the log of the \bfdef{odds ratio} $P_1/P_0=P1/(1-P_1)$:
\bdm
\hat{y}^*(\vec{x})=\vec{\beta}\tr \vec{x} = \ln\left(\frac{P_1}{P_0}\right)
\edm
\ei
}

}


%###################################################
\frame{\frametitle{Example: naive OLS-estimation (RP student interviews)}
%###################################################
\placebox[center]{0.25}{0.60}
{\figSimple{0.55\textwidth}{figsRegr/regr_logistic_WS1516cum_f_eng.png}}

\placebox[center]{0.75}{0.60}
{\figSimple{0.55\textwidth}{figsRegr/regr_logistic_WS1516cum_eng.png}}

\placebox[center]{0.50}{0.20}
{\parbox{\textwidth}{
\bi
\item Alternatives: $i=1$: motorized and $i=2$ (not)
\item Intermediate variable estimated by percentaged choices: $y^*=\ln(f_1/(1-f_1))$
\item Model: Log. regression, 
$\hat{y}^*(x_1)=\beta_0  + \beta_1 x_1 $
\item OLS Estimation: $\beta_0=-0.58, \quad \beta_1=0.79 $
\ei
}}
}



%###################################################
\frame{\frametitle{Method consistent? added
    5$\sup{th}$ data point with f=0.9999}
%###################################################

\placebox[center]{0.25}{0.60}
{\figSimple{0.55\textwidth}{figsRegr/regr_logistic_WS1516cum_alt_f_eng.png}}

\placebox[center]{0.75}{0.60}
{\figSimple{0.55\textwidth}{figsRegr/regr_logistic_WS1516cum_alt_eng.png}}

\placebox[center]{0.50}{0.20}
{\parbox{\textwidth}{
\bi
\item Same model:
 $\hat{y}^*(x_1)=\beta_0 + \beta_1 x_1$
\item New estimation: $\beta_0=-3.12, \quad \beta_1=2.03 $
\item Estimation would fail if $f_1=0$ or =1 $\Rightarrow$ real
  discrete-choice model necessary!
\ei
}}
}


%###################################################
\frame{\frametitle{Comparison: real Maximum-Likelihood (ML) estimation}
%###################################################

% png File von von ~/vorlesungen/Verkehrsoekonometrie_Ma/discrChoice_cc_Levmar/*.eng.gnu
% setze beta1_levmar=-beta1_LogR_OLS
% setze beta2_levmar=-beta0_LogR_OLS
% Vorzeichen: Da delta_{i1}->delta_{i0} statt delta_{i2}->delta_{i2} 
% Reihenfolge/Bezeichnung: inkonsistent historisch
% Im latex File Bez. wie bei OLS-logist Regr in ../skripts/figsRegr/


\fig{0.7\textwidth}{figsRegr/revealedChoiceWS1516cum_2al_4dataPoints_fProb_r.png}
\bi
\item Model: Logit, $V_i(x_1)=\beta_0 \delta_{i1} + \beta_1 x_1 \delta_{i1}$, 
$V_2=0$.
\item Estimation: $\beta_0=-0.50\pm 0.65, \ \beta_1=+0.71\pm 0.30$
\ei

}

%###################################################
\frame{\frametitle{Comparison: real ML estimation
	with added 5$\sup{th}$ data point}
%###################################################

\fig{0.7\textwidth}{figsRegr/revealedChoiceWS1516cum_2al_fProb_r.png}
\bi
\item Same logit model, 
$V_i(x_1)=\beta_0 \delta_{i1} + \beta_1 x_1 \delta_{i1}$, 
$V_2=0$.
\item New estimation: $\beta_0=-0.55\pm 0.63, \ \beta_1=+0.75\pm 0.27$
\ei
}

\end{document}


